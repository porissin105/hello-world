from __future__ import division, print_function, absolute_import
from cifar10 import cifar10_Data
import tensorflow as tf
import numpy as np
import os
import time
import tensorflow.contrib.slim as slim

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'    # close the warning
os.environ['CUDA_VISIBLE_DEVICES'] = '0'    # use gpu 0
config = tf.ConfigProto()
# config.gpu_options.per_process_gpu_memory_fraction = 0.8  # use 40 % gpu memory

model_path = "data "  # save path #data=data address

# Training Parameters
data_sample=50000
epoch = 100                  # 100 runs with 55000 train data
total_batch = 200            # In each runs , train data 55000 = 200 x 275 (total_batch x batch_size)
batch_size = 250
display_step = 50
keep_prob = 0.7

# Learning rate decay
learn_init = 0.05
global_step = tf.Variable(0, trainable=False)
learning_rate = tf.train.exponential_decay(learn_init, global_step, total_batch, 0.9, staircase=True)

# Shuffle coefficient for all data
allran_num = np.arange(0,data_sample)
np.random.shuffle(allran_num)
allInt = allran_num.astype(int)
Indexall = allInt.tolist()
# Shuffle coefficient in train step
ran_num = np.arange(0,data_sample)
np.random.shuffle(ran_num)
Int = ran_num.astype(int)
Index = Int.tolist()

# Load data
trainData, trainlabel, testData, testlabel = cifar10_Data()

# Shuffle
trainData=trainData[Indexall]
trainlabel=trainlabel[Indexall]

# placeholder
X = tf.placeholder(tf.float32, [None, 32,32,3])
Y = tf.placeholder(tf.float32, [None, 10])
is_training = tf.placeholder(tf.bool)


x = tf.convert_to_tensor(X, np.float32)
weight_decay = 0.00001
batch_norm_decay = 0.997
batch_norm_epsilon = 1e-5
batch_norm_scale = True
batch_norm_params = {
      'decay': batch_norm_decay,
      'epsilon': batch_norm_epsilon,
      'scale': batch_norm_scale,
      'updates_collections': tf.GraphKeys.UPDATE_OPS,
      'is_training': is_training,
  }


with slim.arg_scope([slim.conv2d, slim.fully_connected],
      weights_regularizer = slim.l2_regularizer(weight_decay),
      weights_initializer = slim.variance_scaling_initializer(),
      activation_fn = tf.nn.relu,
      normalizer_fn = slim.batch_norm,
      normalizer_params = batch_norm_params):
    with slim.arg_scope([slim.dropout], keep_prob=0.7, is_training=is_training):
        with slim.arg_scope([slim.max_pool2d], kernel_size=[2, 2], stride=[2, 2]):
            with slim.arg_scope([slim.conv2d], padding='SAME'):
                """Traditional Convolution"""
                preconv_1 = slim.conv2d(x, 4, [5, 5])
                preconv_2 = slim.conv2d(x, 16, [5, 5])
                preconv_3 = slim.conv2d(x, 64, [5, 5])

                """Inception Module1"""
                Incept1_a = slim.conv2d(preconv_3, 32, [1, 1])
                Incept1_b1 = slim.conv2d(preconv_3, 16, [1, 1])
                Incept1_b2 = slim.conv2d(Incept1_b1, 32, [3, 3])
                Incept1_c1 = slim.conv2d(preconv_3, 16, [1, 1])
                Incept1_c2 = slim.conv2d(Incept1_c1, 32, [5, 5])
                Incept1_d1 = slim.max_pool2d(preconv_3, stride=[1, 1], padding='SAME')
                Incept1_d2 = slim.conv2d(Incept1_d1, 32, [1, 1])
                Inception1 = tf.concat(axis=3, values=[Incept1_a, Incept1_b2, Incept1_c2, Incept1_d2])

                """Inception Module2"""
                Incept2_a = slim.conv2d(Inception1, 32, [1, 1])
                Incept2_b1 = slim.conv2d(Inception1, 16, [1, 1])
                Incept2_b2 = slim.conv2d(Incept2_b1, 32, [3, 3])
                Incept2_c1 = slim.conv2d(Inception1, 16, [1, 1])
                Incept2_c2 = slim.conv2d(Incept2_c1, 32, [5, 5])
                Incept2_d1 = slim.max_pool2d(Inception1, stride=[1, 1], padding='SAME')
                Incept2_d2 = slim.conv2d(Incept2_d1, 32, [1, 1])
                Inception2 = tf.concat(axis=3, values=[Incept2_a, Incept2_b2, Incept2_c2, Incept2_d2])

                """Global Average Pooling"""
                classes = slim.conv2d(Inception2, 10, [1, 1])  # Change size to number of classes
                global_avgpool = slim.avg_pool2d(classes, kernel_size=[classes.shape[1], classes.shape[2]],
                                                 stride=[1, 1], padding='VALID')  # Global Average Pooling
                output = tf.reshape(global_avgpool, [-1, 10])


# Output layer & Accuracy
prediction = tf.nn.softmax(output)
loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=Y))
correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

# Batch_normalization tensorflow official
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_op)


init = tf.global_variables_initializer()
saver = tf.train.Saver()

best_test_accuracy = 0
count = 0
counter = 0

with tf.Session(config=config) as sess:
    sess.run(init)

    for epoch in range(1, epoch+1):

        result_train = 0
        result_test = 0
        print("--------------Epoch" + str(epoch) + "----------------")
        trainData = trainData[Index]
        trainlabel = trainlabel[Index]
        for step in range(1, total_batch+1):
            sess.run(train_op, feed_dict={X: trainData[0 + (batch_size) * (step - 1):(batch_size) + (batch_size) * (step - 1)],
                                          Y: trainlabel[0 + (batch_size) * (step - 1):(batch_size) + (batch_size) * (step - 1)],
                                          is_training: True})

            if step % display_step == 0:
                acc = sess.run(accuracy, feed_dict={X: trainData[0 + (batch_size) * (step - 1):(batch_size) + (batch_size) * (step - 1)],
                                                    Y: trainlabel[0 + (batch_size) * (step - 1):(batch_size) + (batch_size) * (step - 1)],
                                                    is_training: False})
                print("Step " + str(step) + ", Training Accuracy= " + "{:.4f}".format(acc))


        for x in range(20):
            result_train_1=sess.run(accuracy,
                                    feed_dict={X: trainData[x*2500:(x+1)*2500],
                                               Y: trainlabel[x*2500:(x+1)*2500],
                                               is_training: False})
            result_train = result_train + result_train_1 * (1 / 20)

        for x in range(4):
            result_test_1=sess.run(accuracy,
                                    feed_dict={X: testData[x*2500:(x+1)*2500],
                                               Y: testlabel[x*2500:(x+1)*2500],
                                               is_training: False})
            result_test = result_test + result_test_1 * (1 / 4)

        # early stop
        count = count + 1
        counter = counter + 1
        if best_test_accuracy <= result_test and result_train - result_test < 0.1:
            best_test_accuracy = result_test
            print("***** best test  accuracy :" + "{:.4f}".format(best_test_accuracy) + " ****")
            print("***** best train accuracy :" + "{:.4f}".format(result_train) + " ****")
            count = 0
            save_path = saver.save(sess, model_path, write_meta_graph=False)
        else:
            print("Testing  accuracy=" + "{:.4f}".format(result_test))
            print("Training accuracy=" + "{:.4f}".format(result_train))

        if count == 20:
            break
        if result_train - result_test < 0.1:
            counter = 0
        if counter == 20:
            break

# Restore weights & biases variable
print("----------------------Restore------------------------------")
with tf.Session() as sess:

    saver.restore(sess, model_path)
    result_test = 0
    result_train = 0

    for x in range(20):
        result_train_1 = sess.run(accuracy,
                                  feed_dict={X: trainData[x * 2500:(x + 1) * 2500],
                                             Y: trainlabel[x * 2500:(x + 1) * 2500],
                                             is_training: False})
        result_train = result_train + result_train_1 * (1 / 20)

    for x in range(4):
        result_test_1 = sess.run(accuracy,
                                 feed_dict={X: testData[x * 2500:(x + 1) * 2500],
                                            Y: testlabel[x * 2500:(x + 1) * 2500],
                                            is_training: False})
        result_test = result_test + result_test_1 * (1 / 4)
    print("Best testing  accuracy=" + "{:.4f}".format(result_test))
    print("Best training accuracy=" + "{:.4f}".format(result_train))
